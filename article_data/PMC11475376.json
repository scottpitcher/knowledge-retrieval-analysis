{
    "title": "The Growing Impact of Natural Language Processing in Healthcare and Public Health",
    "abstract": "Natural Language Processing (NLP) is a subset of Artificial Intelligence, specifically focused on understanding and generating human language. NLP technologies are becoming more prevalent in healthcare and hold potential solutions to current problems. Some examples of existing and future uses include: public sentiment analysis in relation to health policies, electronic health record (EHR) screening, use of speech to text technology for extracting EHR data from point of care, patient communications, accelerated identification of eligible clinical trial candidates through automated searches and access of health data to assist in informed treatment decisions. This narrative review aims to summarize the current uses of NLP in healthcare, highlight successful implementation of computational linguistics-based approaches, and identify gaps, limitations, and emerging trends within the subfield of NLP in public health. The online databases Google Scholar and PubMed were scanned for papers published between 2018 and 2023. Keywords \u201cNatural Language Processing, Health Policy, Large Language Models\u201d were utilized in the initial search. Then, papers were limited to those written in English. Each of the 27 selected papers was subject to careful analysis, and their relevance in relation to NLP and healthcare respectively is utilized in this review. NLP and deep learning technologies scan large datasets, extracting valuable insights in various realms. This is especially significant in healthcare where huge amounts of data exist in the form of unstructured text. Automating labor intensive and tedious tasks with language processing algorithms, using text analytics systems and machine learning to analyze social media data and extracting insights from unstructured data allows for better public sentiment analysis, enhancement of risk prediction models, improved patient communication, and informed treatment decisions. In the recent past, some studies have applied NLP tools to social media posts to evaluate public sentiment regarding COVID-19 vaccine use. Social media data also has the capacity to be harnessed to develop pandemic prediction models based on reported symptoms. Furthermore, NLP has the potential to enhance healthcare delivery across the globe. Advanced language processing techniques such as Speech Recognition (SR) and Natural Language Understanding (NLU) tools can help overcome linguistic barriers and facilitate efficient communication between patients and healthcare providers.",
    "body": "Natural Language Processing (NLP) is a subset of Artificial Intelligence, specifically focused on understanding and generating human language. NLP technologies are becoming more prevalent in healthcare and hold potential solutions to current problems. Some examples of existing and future uses include: public sentiment analysis in relation to health policies, electronic health record (EHR) screening, use of speech to text technology for extracting EHR data from point of care, patient communications, accelerated identification of eligible clinical trial candidates through automated searches and access of health data to assist in informed treatment decisions. This narrative review aims to summarize the current uses of NLP in healthcare, highlight successful implementation of computational linguistics-based approaches, and identify gaps, limitations, and emerging trends within the subfield of NLP in public health. The online databases Google Scholar and PubMed were scanned for papers published between 2018 and 2023. Keywords \u201cNatural Language Processing, Health Policy, Large Language Models\u201d were utilized in the initial search. Then, papers were limited to those written in English. Each of the 27 selected papers was subject to careful analysis, and their relevance in relation to NLP and healthcare respectively is utilized in this review. NLP and deep learning technologies scan large datasets, extracting valuable insights in various realms. This is especially significant in healthcare where huge amounts of data exist in the form of unstructured text. Automating labor intensive and tedious tasks with language processing algorithms, using text analytics systems and machine learning to analyze social media data and extracting insights from unstructured data allows for better public sentiment analysis, enhancement of risk prediction models, improved patient communication, and informed treatment decisions. In the recent past, some studies have applied NLP tools to social media posts to evaluate public sentiment regarding COVID-19 vaccine use. Social media data also has the capacity to be harnessed to develop pandemic prediction models based on reported symptoms. Furthermore, NLP has the potential to enhance healthcare delivery across the globe. Advanced language processing techniques such as Speech Recognition (SR) and Natural Language Understanding (NLU) tools can help overcome linguistic barriers and facilitate efficient communication between patients and healthcare providers. \nWhat do we already know about this topic?\n Natural Language Processing (NLP) technologies are becoming more prevalent in healthcare and hold potential solutions to current challenges (eg,: electronic health record screening, gaps in medical record data, patient communications). \nHow does your research contribute to the field?\n Our narrative review summarizes the current uses of NLP in healthcare, highlight successful implementation of computational linguistics-based approaches, and identify gaps, limitations, and emerging trends within the subfield of NLP in public health and healthcare. \nWhat are your research\u2019s implications toward theory, practice, or policy?\n Continued development of NLP applications in healthcare will contribute to several improvements in areas such as public sentiment analysis, enhancement of risk prediction models, and improved patient communication in the future. Computational Linguistics (CL) and Natural Language Processing (NLP) are fields at the intersection of computer science and linguistics. CL is defined as the development and application of computational models to understand, analyze, and process human language. NLP is a subfield of computational linguistics, focused on Artificial Intelligence (AI) models that interpret and generate human language. NLP utilizes AI-based techniques to extract information from large text samples, derive its meaning, and respond to human inputs. Both play an important role in advancing methods by which computers interact with human language. The primary objective of this paper is to examine the role of Computational Linguistics (CL) in addressing critical challenges in the fields of public health and healthcare. By conducting an in-depth analysis of existing literature, this review will summarize the current uses of NLP applications in healthcare. This review will also highlight successful implementation of computational linguistics-based approaches. Gaps, limitations, and emerging trends within this sub-field will be addressed, allowing for a comprehensive analysis of NLP\u2019s potential integration into healthcare. This review will provide analysis of the solutions that CL and NLP provide in public health advertising, medical record analysis, and patient communication. The broader goal of our review is to increase accessibility of information that will inform future research, policy, and implementation of NLP/CL techniques in healthcare. NLP and CL offer novel opportunities for medical professionals to analyze massive amounts of unstructured data (eg, clinical notes\n1\n, medical records, public health literature, public reaction to health policies on social media), facilitating valuable insights. \u201c80% of medical data remains unstructured and untapped after it is created (eg, text, image, signal, etc.) (Figure 1) Since it is hard to handle this type of data, it tends to be ignored, unsaved, or abandoned in most medical centers for a long time.\u201d\n18\n As the digital era continues to advance the methods by which information can be generated, linguistic and computational tools have the potential to enhance current practices. NLP technology has the potential to enhance the efficacy of diagnostics and suggest potential treatment and patient care plans. These examples showcase only a portion of the potential impact that NLP could have in public health and related fields. By analyzing the connections between computation, language, and healthcare, this paper provides a synopsis of the intersection of computational linguistics and medicine. Some of the potential advances enabled by application of NLP techniques and lists some of the tools used. The material utilized in this narrative review was sourced from the online databases Google Scholar and PubMed (as they have the largest scope and are most relevant to NLP), through searches for papers published between 2018 and 2023. The following keywords were utilized: natural language processing, health policy, large language models. The search criteria was limited to papers written in English with a focus on healthcare and advertising. A brief disclaimer: the company now known as \u201cX\u201d is referred to as Twitter throughout the paper, as the studies examined were published before the rebranding of the company. This literature review found 49 articles meeting the study criteria keywords during the study time frame gathered through searches on PubMed and Google Scholar. All irrelevant results (unrelated to healthcare, NLP, and public opinion) and duplicate articles were removed, leaving n\u2009=\u200927 papers1-27. These articles all met the eligibility requirements for this review, allowing for their inclusion. NLP technologies, especially following the COVID-19 pandemic, have been utilized to analyze public opinion on various health policies. COVID-19, in particular, demonstrated significant variation in public opinion with regional variation in vaccination and mask policies. An international, comprehensive study conducted by the University of Utah over Reddit servers in Australia, Canada, the United States, and the United Kingdom, delved into opinions posted on forums and discussions, allowing for insights into what these users were feeling.\n13\n Over 84\u2009000 posts and 1\u2009000\u2009000 comments (Feb 2020-Nov 2020) were explored by latent dirichlet allocation (LDA) software. LDA is a common technique in NLP that allows for the inference of topics from large bodies of text. Important distinctions in the discussion of similar topics between the regions on Reddit were revealed. The study showed that an NLP based approach could offer deeper perspectives than survey-based approaches. NLP was able to do so because of the sheer amount of data able to be screened. A similar study was funded by the Japan Science and Technology Agency, where NLP was utilized to learn more about public views on the COVID vaccine.\n23\n Using an NLP model, 144\u2009101 tweets from Japan (August 2020-June 2021) were reviewed, helping professionals identify relationships between public views and infection, death, and vaccination levels. The LDA model identified roughly 85% of the tweets to express neutral opinions on vaccination, with negative opinions outweighing positive for the remaining 15%. Keywords such as \u201creceive, side effect, prevention, danger, government, infect,\u201d allowed the model to separate the neutral, negative, and positive reviews cited above. The NLP model, in this study, allowed for screening and sorting of massive amounts of data, which would have been less efficient if done by human experts . There are numerous studies supporting this claim, particularly in relation to COVID-19 and Table 1 depicts the data analyzed by similar methods, and results/conclusions of each. Summary of studies related to COVID-19 identified in the literature review. Collectively, the above studies showcase NLP\u2019s existing use in analysis of public opinion on health policy, being able to analyze millions of data points across these cases during a pandemic when so many people utilized social media to convey their thoughts. Across the majority of these studies, trends relating to both positive and negative posts were correlated with rates of infection and death, along with efficacy of various vaccines and their development. Identification of thematic trends in social media posts tracks public opinion and allows for in-depth responses from public health officials that achieve higher satisfaction and compliance. These uses of LDA, topic modeling, and methods such as unsupervised ML, coupled with sentiment analysis, depict NLP\u2019s adaptability in various research contexts. Through analysis of papers using these methods, insights into public response to realms beyond COVID-19 policy can be made, although COVID-19 response contains the most prominent examples, as the pandemic provided a need for NLP technology capable of such large-scale analysis. Natural language processing can be utilized for analysis of various data types, including textual clinical data and patient electronic health records. Manual review of patient records is a tedious process. Trends and similarities in clinical texts correlate with risk of future medical complications and hospitalization. With NLP of these textual bodies, predictive models can be created, scanning patient clinical data and forecasting admission into medical facilities. Across various studies, NLP methods have consistently proven their value in enhancing predictive performance. One such study delves into the incorporation of NLP to construct a comprehensive analytical framework.\n22\n Focusing on accurate prediction of mortality outcomes in ICU patients, the combination of NLP derived keywords and terms consistently enhanced model performance and increased the area under the receiver operating characteristic curve (AUC) from 0.831 to 0.922 (AUC is a measure of predictive accuracy with values ranging from 0 to 1. A higher value indicates higher predictive accuracy).\n27\n Such a significant jump in AUC shows the value of NLP in predictive risk modeling, a tendency not exclusive to ICU mortality models. In an NLP model designed to identify risk factors correlating with lower back pain from the electronic medical record of patients, only 347 out of the 2749 health records of patients exhibiting back pain sampled contained one or more of the keywords related to said risks.\n15\n Despite downsizing, the input dataset still resulted in a 2% increase in the AUC. While this sort of sampling may result in biases of the model in analysis\u2014being more likely to identify text similar to its limited training data\u2014the fact that AUC was able to improve in these circumstances shows the promise of the NLP techniques utilized in this case. Another example of natural language processing of medical records can be found in a study relating to gender identity.\n11\n Precise gender identity identification is essential in ensuring equitable healthcare, particularly within gender-diverse populations. Electronic Health Records (EHR) fields often contain incomplete or insufficient gender information; through use of gender-related keywords and implementation of NLP and deep learning, the group constructed an accurate predictive model for patient gender identity. This proved that NLP models, integrated with deep learning techniques, can be utilized for efficient prediction of patient gender identity when trained on limited EHR data. Initial scanning techniques in this subfield are often generalizable, as the processes for large-scale textual file analysis tend to be fairly consistent across various applications of it. Similar methods were utilized in a study of 76\u2009479 patients over age 65 with multimorbidity (more than one incurable illness), where NLP was used to extract psychosocial factors (eg, solation, housing, financial insecurity, stress) from chart notes, and predict their impact on 1-year hospitalization. Other variables were isolated from the EHR of said patients, and logistic regression was used to predict the likelihood of health issues in relation to these factors. The model found that patients with any of the aforementioned psychosocial factors had higher baseline utilization of healthcare facilities, and a greater number of chronic illnesses. Predictive models that did not evaluate with consideration to psychosocial factors performed worse, and had overall less efficient discrimination. NLP of tens of thousands of chart notes allowed for the identification of said factors, and therefore improved model performance.\n8\n NLP technology has significant potential to address issues within clinical settings as well. Some of these issues were especially evident during the pandemic, particularly involving the identification and understanding of COVID-19 symptoms within clinics.\n5\n During the peak of the pandemic healthcare centers were overworked, and often patient symptoms were not accurately transcribed into their records. Any NLP-based analysis of these resulting skewed EHR records would be restricted by the information entered. In addition to this, lab tests were scarcely available, so diagnosis often depended on symptoms observed during clinic visits. These symptoms are diverse, including the common fever, cough, sore throat, and dyspnea. However, there are less common symptoms associated with COVID, including insomnia, rhinorrhea, nausea, myalgias, and many more. The study proposed to instead examine audio recordings of patient visits, providing a comprehensive and complete source of symptomatic data for NLP technologies to examine. With audio recordings of patient interaction, lesser known symptoms and keywords that might be missed during transcription to the EHR can be recorded. The combination of audio data and EHR data would improve accuracy of categorization for diagnosis, accounting for the limitations of the health record. In addition to audio recordings of patient interactions, the authors of this study seek to acquire data from telehealth appointments, as well as from voice-activated systems including the Amazon Alexa, Google Home, and Siri. With audio transcriptions being a primary source of data, there are ample repositories to pull from to train predictive models with presumably high accuracy (due to the sheer amount of available data). While there are limitations to such methods, including the need for diligent protection of patient information and complications involved with accessing existing patient recordings, audio transcription-based NLP allows for data analysis on a greater scale than traditional methods. NLP presents significant potential in the evolution of patient communication going forward. It enables the analysis and understanding of complex language for improved interactions between healthcare providers and patients. Large Language Models (LLMs) are NLP models with advanced comprehension of human language, being trained on massive amounts of textual data and containing over 100\u2009billion parameters. These models can be trained to understand and simplify the complex language associated with doctor-patient communications. A study published in the Yale Journal of Biology and Medicine describes how electronic health information (eg, imaging reports) has become more accessible to patients due to the 21st Century Cures Act, passed in December of 2016\u2014which, aside from speeding up various regulatory processes, \u201cimproved patient access to their electronic health information,\u201d including imaging reports.\n3\n Despite the benefits of this act, these reports are complex, being intended for internal communication between doctors. They are often specific and detailed, with medical terms typically beyond the comprehension of most patients. Patient understanding of their health is crucial in improving future treatment, decreasing hospitalizations, and improving long term wellness; this lack of patient comprehension and gap in communication undoubtedly has a negative affect on future health outcomes. There are proposed solutions to this problem, including summary statements, second reports, and providing the radiologist\u2019s phone number. However, these all complicate the radiologist\u2019s tasks, distracting them from already strenuous daily requirements. LLMs such as ChatGPT by OpenAI\u2014alongside similar deep learning models including Google\u2019s Bard AI and Microsoft Bing\u2014driven by NLP have the ability to simplify imaging reports without causing the aforementioned disruptions for the radiologist. Approaches incorporating DL-NLP have already been utilized in \u201cidentifying critical findings, categorizing oncologic responses, finding follow-up recommendations, identifying pulmonary emboli, detecting complications of stroke, and classifying epilepsy brain MRIs.\u201d\n3\n Between the numerous studies conducted in this area, LLMs show potential in report simplification for patient communications, though expert verification of the output is still necessary before large-scale implementation. The various studies that showcase NLP technologies\u2019 roles in mass analysis of public opinion on health policies are linked in both conclusions and methodology. They depict NLP\u2019s ability to process large volumes of data from public forums such as Reddit and Twitter, and uncover differing sentiments from across the globe. LDA techniques, topic modeling, unsupervised machine learning, were coupled with sentiment analysis in these studies, allowing for efficient screening and sorting of the vast datasets provided by these social media sources. Despite these evident advantages, there are risks associated with sole reliance on NLP analyses. Unrepresentative keyword input, or bias in training datasets can distort the outcomes of predictive models. These studies emphasize the need for algorithm refinement to eliminate biases. A balance between automated predictive models and expert verification would ensure reliability of predictive outputs. Transparency in methodology, frequent algorithmic performance evaluation, and collaboration between researchers is necessary to increase the accuracy and consistency of NLP-based models, especially when data found on social media platforms is inherently subjective. NLP could facilitate the manual review of patient and electronic health records is an unnecessarily time-consuming process. Just as we extract insights from textual bodies and symptoms, NLP is able to scan for keywords associated with various diseases. Predictive risk assessment models can be trained based on this NLP keyword detection, with greater accuracy than standard neural networks. While the advantages of NLP based deep learning approaches are numerous, training based on EHR datasets poses issues in relation to patient data and confidentiality. Vulnerabilities in NLP models could result in unintentional data breaches, potentially compromising patient information. One potential solution to address this moving forward is the incorporation of blockchain technology, which would provide security for any EHR utilized. In simple terms, blockchain is a form of storage, weaving secured data into interconnected sequences.\n28\n Blockchain networks contain a variety of cryptographic techniques and consensus mechanisms in order to ensure data integrity. Patient information can be encrypted and stored, and with the use of randomly generated private keys being required for access, confidentiality will be preserved. This technology would mitigate many vulnerabilities associated with models utilizing electronic health records, resulting in enhanced security for trials involving them. Despite the advantages associated with EHR analysis, these records are frequently imperfect, with potential keywords associated with specific diseases not being entered. However, audio recordings, and subsequent transcriptions of patient interactions with doctors, eliminate this issue. If EHR entries are made with a complete collection of the keywords mentioned by patients and doctors during appointments, predictive models utilizing NLP will be able to function at a higher level. Records containing information relevant to specific diseases would be able to be identified via NLP, allowing for the patient\u2019s risks to be properly assessed. However, similar challenges arise with audio recordings as do with any EHR analysis. Patient confidentiality must be protected, and advanced security measures must be taken to do so. In addition to this, patient consent in being recorded in the first place is necessary, and measures would have to be taken to be as transparent as possible with every patient whose health records would be affected by such methods. As highlighted in the review, electronic health reports are becoming more and more accessible to patients. While access to these reports is important, understanding them can be quite complicated, as they are abstruse in nature. Imaging reports are perfect examples of this; originally meant for internal communication within hospitals and between doctors, terms utilized in them would be lost on the majority of patients. LLMs such as ChatGPT can simplify this complex language into phrases and expressions understandable by the average person. Overall ease of patient communication would be heightened through the utilization of LLMs, as general understanding of specific health conditions and treatments would also be increased. Though, as with any AI model, expert verification is necessary before simplified \u201ctranslations\u201d are provided for patients. Accuracy must be a priority when using LLMs in such a crucial manner, as a mistake in wording could result in misleading a patient in regards to their reports. Integration of LLMs into healthcare communications demands precise validation to avoid patient misinterpretations. Table 2 below outlines how NLP has the potential to advance health care systems globally by addressing and identifying health disparities among sub-groups of the population, identifying high risk patients, addressing issues of gender disparity, enhancing clinical education related to different medical conditions through better diagnostics and addressing issues that will become increasingly important in the future such as managing the patient with multi-morbid conditions. Summary of studies evaluating NLP potential in advancing health care systems This narrative review has limitations that should be considered when interpreting its conclusions. Firstly, the search was limited to papers published between 2018 and 2023, which may have excluded earlier relevant papers on NLP in healthcare. Additionally, the rapid evolution of NLP technologies means that some of the reviewed studies may not reflect the most current advancements in the field. Moreover, the use of only two databases (Google Scholar and PubMed) may have limited the scope of included studies. Lastly, as this is a literature review rather than a systematic review or meta-analysis, it may not capture all relevant studies in the field. While the studies discussed in this narrative review have shown the many uses of NLP and Deep Learning in public health and medicine, swift implementation of these technologies will be beneficial. The advantages of such programs must not be understated, and despite the complications involved in their use, the need for such programs is increasing.\n29\n Widely used products from technology giants such as Meta (FB), Alphabet (Google) and Microsoft (Bing) are offering built-in NLP tools to customers. NLP powered chatbots and personal assistants are becoming increasingly common. Much of the efficacy of NLP is based on the ability of algorithms to scan large datasets efficiently, understand human language (whether spoken or written) and mine scientific literature for insights. This is especially significant in healthcare, where huge amounts of data exists as unstructured text. Analyzing and interpreting data from such voluminous unstructured datasets offers many opportunities for identifying patterns, gaining insights and a resultant improvement in patient outcomes. These technologies also have certain limitations that will need to be addressed before widespread adoption in healthcare settings. Technology in healthcare should be held to more stringent standards as compared to other areas. Access to high quality datasets for training the NLP models is critical to their development. Recognizing biases in data and eliminating these during training will also be crucial. Concerns about data use and privacy (HIPAA guidelines) will also need to be addressed at a policy level. Large scale adoption of NLP will require huge computational capabilities. There will also be a need to integrate these tools with existing IT systems and Artificial Intelligence (AI) models. Such limitations notwithstanding, NLP offers the promise of numerous benefits for public health officials, healthcare practitioners as well as patients. Increased focus on addressing the shortcomings will, in the long term, yield significant improvements across many realms of healthcare. None. Author Contribution Statements: Aadit Jerfy conceptualized the paper, wrote the first draft, and conducted the literatire review. Owen Selden co-wrote the first draft and contributed to the methodology of the review. Rajesh Balkrishnan conceptualized the paper, developed the methodology and revised the first draft of manuscript. The author(s) declared the following potential conflicts of interest with respect to the research, authorship, and/or publication of this article: Dr. Balkrishnan is currently a consultant for Otsuka Pharmaceuticals Inc. None of the other authors have any pertinent conflict of interests. Funding: The author(s) received no financial support for the research, authorship, and/or publication of this article. Ethical/Consent Statement: Our study did not require an ethical board approval because it is a review. ORCID iD: Rajesh Balkrishnan \nhttps://orcid.org/0000-0001-9363-9257"
}